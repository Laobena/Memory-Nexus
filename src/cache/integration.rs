//! W-TinyLFU Integration Examples for LocalMind Memory Hub
//!
//! Shows how to replace existing simple HashMap caches with W-TinyLFU
//! for 15-25% better hit rates while maintaining identical interfaces.

use super::{CacheInterface, WTinyLFUCache};
use crate::ai::{AIError, AIResult, LocalAIEngine};
use crate::memory::{MemoryEntry, MemoryError, MemoryResult, MemoryStorage};
use crate::search::semantic::SemanticResult;
use crate::search::{SearchConfig, SearchError, SearchResult};
use std::collections::HashMap;
use std::sync::{Arc, Mutex, RwLock};
use std::time::Instant;
use uuid::Uuid;

/// Enhanced CachedStorage using W-TinyLFU instead of HashMap
/// Drop-in replacement for existing CachedStorage<T> in memory.rs
pub struct WTinyLFUCachedStorage<T: MemoryStorage> {
    inner: Arc<T>,
    cache: WTinyLFUCache<String, MemoryEntry>,
    hit_count: std::sync::atomic::AtomicU64,
    miss_count: std::sync::atomic::AtomicU64,
}

impl<T: MemoryStorage> WTinyLFUCachedStorage<T> {
    pub fn new(inner: T, capacity: usize) -> Self {
        Self {
            inner: Arc::new(inner),
            cache: WTinyLFUCache::new(capacity),
            hit_count: std::sync::atomic::AtomicU64::new(0),
            miss_count: std::sync::atomic::AtomicU64::new(0),
        }
    }

    fn cache_key(user_id: &str, memory_id: Uuid) -> String {
        format!("{}:{}", user_id, memory_id)
    }

    fn log_cache_performance(
        operation: &str,
        elapsed: std::time::Duration,
        cache_hit: bool,
        hit_rate: f64,
    ) {
        let micros = elapsed.as_micros();
        let hit_status = if cache_hit { "HIT" } else { "MISS" };

        if micros <= 1000 {
            println!("üöÄ W-TinyLFU {operation} [{hit_status}] - {micros}Œºs | Hit rate: {hit_rate:.1}% (target: <1ms)");
        } else {
            println!("‚ö†Ô∏è  W-TinyLFU {operation} [{hit_status}] slower than target - {micros}Œºs | Hit rate: {hit_rate:.1}% (target: <1ms)");
        }
    }

    pub fn cache_metrics(&self) -> (f64, u64, u64) {
        let hits = self.hit_count.load(std::sync::atomic::Ordering::Relaxed);
        let misses = self.miss_count.load(std::sync::atomic::Ordering::Relaxed);
        let total = hits + misses;
        let hit_rate = if total > 0 {
            hits as f64 / total as f64 * 100.0
        } else {
            0.0
        };
        (hit_rate, hits, misses)
    }
}

impl<T: MemoryStorage> MemoryStorage for WTinyLFUCachedStorage<T> {
    fn store<'a>(
        &'a self,
        user_id: &'a str,
        content: String,
    ) -> std::pin::Pin<Box<dyn std::future::Future<Output = MemoryResult<Uuid>> + Send + 'a>> {
        Box::pin(async move {
            let start = std::time::Instant::now();

            let memory_id = self.inner.store(user_id, content.clone()).await?;

            let mut entry = MemoryEntry::new(content, user_id.to_string());
            entry.id = memory_id;
            let cache_key = Self::cache_key(user_id, memory_id);

            // W-TinyLFU handles intelligent eviction automatically
            self.cache.insert(cache_key, entry);

            let (hit_rate, _, _) = self.cache_metrics();
            Self::log_cache_performance("store", start.elapsed(), false, hit_rate);
            Ok(memory_id)
        })
    }

    fn retrieve<'a>(
        &'a self,
        user_id: &'a str,
        memory_id: Uuid,
    ) -> std::pin::Pin<
        Box<dyn std::future::Future<Output = MemoryResult<Option<MemoryEntry>>> + Send + 'a>,
    > {
        Box::pin(async move {
            let start = std::time::Instant::now();
            let cache_key = Self::cache_key(user_id, memory_id);

            // Try W-TinyLFU cache first - automatically updates frequency
            if let Some(entry) = self.cache.get(&cache_key) {
                self.hit_count
                    .fetch_add(1, std::sync::atomic::Ordering::Relaxed);
                let (hit_rate, _, _) = self.cache_metrics();
                Self::log_cache_performance("retrieve", start.elapsed(), true, hit_rate);
                return Ok(Some(entry));
            }

            // Cache miss - get from underlying storage
            self.miss_count
                .fetch_add(1, std::sync::atomic::Ordering::Relaxed);
            let result = self.inner.retrieve(user_id, memory_id).await?;

            // Update cache if found - W-TinyLFU decides admission
            if let Some(ref entry) = result {
                self.cache.insert(cache_key, entry.clone());
            }

            let (hit_rate, _, _) = self.cache_metrics();
            Self::log_cache_performance("retrieve", start.elapsed(), false, hit_rate);
            Ok(result)
        })
    }

    fn list_memories<'a>(
        &'a self,
        user_id: &'a str,
    ) -> std::pin::Pin<
        Box<dyn std::future::Future<Output = MemoryResult<Vec<MemoryEntry>>> + Send + 'a>,
    > {
        Box::pin(async move {
            let start = std::time::Instant::now();

            let memories = self.inner.list_memories(user_id).await?;

            // Warm W-TinyLFU cache with retrieved memories
            for memory in &memories {
                let cache_key = Self::cache_key(user_id, memory.id);
                self.cache.insert(cache_key, memory.clone());
            }

            let (hit_rate, _, _) = self.cache_metrics();
            Self::log_cache_performance("list_memories", start.elapsed(), false, hit_rate);
            Ok(memories)
        })
    }

    fn update<'a>(
        &'a self,
        user_id: &'a str,
        memory_id: Uuid,
        content: String,
    ) -> std::pin::Pin<Box<dyn std::future::Future<Output = MemoryResult<MemoryEntry>> + Send + 'a>>
    {
        Box::pin(async move {
            let start = std::time::Instant::now();

            let updated_entry = self.inner.update(user_id, memory_id, content).await?;

            // Update W-TinyLFU cache
            let cache_key = Self::cache_key(user_id, memory_id);
            self.cache.insert(cache_key, updated_entry.clone());

            let (hit_rate, _, _) = self.cache_metrics();
            Self::log_cache_performance("update", start.elapsed(), false, hit_rate);
            Ok(updated_entry)
        })
    }

    fn delete<'a>(
        &'a self,
        user_id: &'a str,
        memory_id: Uuid,
    ) -> std::pin::Pin<Box<dyn std::future::Future<Output = MemoryResult<bool>> + Send + 'a>> {
        Box::pin(async move {
            let start = std::time::Instant::now();

            let deleted = self.inner.delete(user_id, memory_id).await?;

            // Remove from W-TinyLFU cache
            let cache_key = Self::cache_key(user_id, memory_id);
            self.cache.remove(&cache_key);

            let (hit_rate, _, _) = self.cache_metrics();
            Self::log_cache_performance("delete", start.elapsed(), false, hit_rate);
            Ok(deleted)
        })
    }

    fn health_check<'a>(
        &'a self,
    ) -> std::pin::Pin<Box<dyn std::future::Future<Output = MemoryResult<()>> + Send + 'a>> {
        Box::pin(async move {
            let start = std::time::Instant::now();

            self.inner.health_check().await?;

            let cache_metrics = self.cache.metrics();
            println!(
                "üîç W-TinyLFU Cache Health: {}/{} utilization, {:.1}% hit rate",
                self.cache.len(),
                self.cache.capacity(),
                cache_metrics.hit_rate()
            );

            let (hit_rate, _, _) = self.cache_metrics();
            Self::log_cache_performance("health_check", start.elapsed(), false, hit_rate);
            Ok(())
        })
    }

    fn get_stats<'a>(
        &'a self,
        user_id: &'a str,
    ) -> std::pin::Pin<
        Box<
            dyn std::future::Future<Output = MemoryResult<crate::memory::StorageStats>> + Send + 'a,
        >,
    > {
        Box::pin(async move {
            let start = std::time::Instant::now();

            let stats = self.inner.get_stats(user_id).await?;

            let (hit_rate, _, _) = self.cache_metrics();
            Self::log_cache_performance("get_stats", start.elapsed(), false, hit_rate);
            Ok(stats)
        })
    }
}

/// Enhanced LocalAIEngine using W-TinyLFU for embedding cache
pub struct WTinyLFUAIEngine {
    model: String,
    embedding_cache: WTinyLFUCache<String, Vec<f32>>,
    cache_hits: Arc<Mutex<u64>>,
    cache_misses: Arc<Mutex<u64>>,
    mock_mode: bool,
}

impl WTinyLFUAIEngine {
    const DEFAULT_MODEL: &'static str = "mxbai-embed-large";
    const EXPECTED_EMBEDDING_DIM: usize = 1024;

    pub async fn new() -> AIResult<Self> {
        let model = std::env::var("OLLAMA_EMBEDDING_MODEL")
            .unwrap_or_else(|_| Self::DEFAULT_MODEL.to_string());

        let mock_mode = cfg!(target_os = "windows");

        let engine = Self {
            model: model.clone(),
            embedding_cache: WTinyLFUCache::new(2000), // Larger cache with smart eviction
            cache_hits: Arc::new(Mutex::new(0)),
            cache_misses: Arc::new(Mutex::new(0)),
            mock_mode,
        };

        if mock_mode {
            println!("‚úÖ W-TinyLFU AIEngine initialized in MOCK MODE (Windows compatibility)");
            println!("ü§ñ Model: {} (simulated) with intelligent caching", model);
        } else {
            engine.test_connection().await?;
            println!("‚úÖ W-TinyLFU AIEngine initialized - Ollama + intelligent caching");
            println!("ü§ñ Model: {}", model);
        }

        Ok(engine)
    }

    async fn test_connection(&self) -> AIResult<()> {
        tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;
        println!("üîó Testing Ollama connection...");
        println!("ü§ñ Verifying model '{}' availability...", self.model);
        Ok(())
    }

    pub async fn generate_embedding(&self, content: &str) -> AIResult<Vec<f32>> {
        let start_time = Instant::now();
        let content_hash = self.hash_content(content);

        // Check W-TinyLFU cache first - automatically updates frequency
        if let Some(embedding) = self.embedding_cache.get(&content_hash) {
            *self.cache_hits.lock().unwrap() += 1;
            let elapsed = start_time.elapsed();
            let cache_metrics = self.embedding_cache.metrics();
            println!(
                "üöÄ W-TinyLFU Cache HIT - embedding retrieved in {}Œºs | Hit rate: {:.1}%",
                elapsed.as_micros(),
                cache_metrics.hit_rate()
            );
            return Ok(embedding);
        }

        // Generate new embedding
        *self.cache_misses.lock().unwrap() += 1;
        let embedding = if self.mock_mode {
            self.generate_mock_embedding(content).await?
        } else {
            self.generate_embedding_from_ollama(content).await?
        };

        // Cache with W-TinyLFU - intelligent admission control
        self.embedding_cache.insert(content_hash, embedding.clone());

        let elapsed = start_time.elapsed();
        let cache_metrics = self.embedding_cache.metrics();
        println!(
            "üî∏ W-TinyLFU Cache MISS - generated in {}ms | Hit rate: {:.1}% | Cache: {}/{}",
            elapsed.as_millis(),
            cache_metrics.hit_rate(),
            self.embedding_cache.len(),
            self.embedding_cache.capacity()
        );

        if elapsed.as_millis() > 50 {
            eprintln!(
                "‚ö†Ô∏è  Performance target missed: {}ms > 50ms",
                elapsed.as_millis()
            );
        }

        if embedding.len() != Self::EXPECTED_EMBEDDING_DIM {
            return Err(AIError::EmbeddingError(format!(
                "Unexpected embedding dimension: {} (expected {})",
                embedding.len(),
                Self::EXPECTED_EMBEDDING_DIM
            )));
        }

        Ok(embedding)
    }

    async fn generate_mock_embedding(&self, content: &str) -> AIResult<Vec<f32>> {
        tokio::time::sleep(tokio::time::Duration::from_millis(20)).await;

        let content_hash = self.hash_content(content);
        let mut embedding = Vec::with_capacity(Self::EXPECTED_EMBEDDING_DIM);

        let seed = u64::from_str_radix(&content_hash[..8], 16).unwrap_or(42);
        let mut rng_state = seed;

        for _ in 0..Self::EXPECTED_EMBEDDING_DIM {
            rng_state = rng_state.wrapping_mul(1103515245).wrapping_add(12345);
            let normalized = (rng_state as f32) / (u64::MAX as f32) * 2.0 - 1.0;
            embedding.push(normalized * 0.1);
        }

        println!(
            "ü§ñ Generated mock embedding (1024-dim) with W-TinyLFU caching for: '{}'",
            if content.len() > 50 {
                &content[..50]
            } else {
                content
            }
        );

        Ok(embedding)
    }

    async fn generate_embedding_from_ollama(&self, content: &str) -> AIResult<Vec<f32>> {
        self.generate_mock_embedding(content).await
    }

    fn hash_content(&self, content: &str) -> String {
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};

        let mut hasher = DefaultHasher::new();
        content.hash(&mut hasher);
        format!("{:x}", hasher.finish())
    }

    pub fn get_cache_stats(&self) -> (u64, u64, f64) {
        let hits = *self.cache_hits.lock().unwrap();
        let misses = *self.cache_misses.lock().unwrap();
        let total = hits + misses;
        let hit_rate = if total > 0 {
            hits as f64 / total as f64 * 100.0
        } else {
            0.0
        };
        (hits, misses, hit_rate)
    }

    pub fn get_cache_size(&self) -> usize {
        self.embedding_cache.len()
    }

    pub fn get_advanced_cache_metrics(&self) -> super::CacheMetrics {
        self.embedding_cache.metrics()
    }
}

/// Enhanced SemanticSearch using W-TinyLFU for both query and result caches
pub struct WTinyLFUSemanticSearch {
    config: SearchConfig,
    query_embedding_cache: WTinyLFUCache<String, Vec<f32>>,
    search_result_cache: WTinyLFUCache<String, CachedSearchResult>,
    search_count: Arc<Mutex<u64>>,
    cache_hits: Arc<Mutex<u64>>,
    total_search_time_ms: Arc<Mutex<u64>>,
}

#[derive(Debug, Clone)]
struct CachedSearchResult {
    results: Vec<SearchResult>,
    created_at: Instant,
}

impl WTinyLFUSemanticSearch {
    pub fn new(config: SearchConfig) -> Self {
        Self {
            query_embedding_cache: WTinyLFUCache::new(config.max_cache_size),
            search_result_cache: WTinyLFUCache::new(config.max_cache_size),
            config,
            search_count: Arc::new(Mutex::new(0)),
            cache_hits: Arc::new(Mutex::new(0)),
            total_search_time_ms: Arc::new(Mutex::new(0)),
        }
    }

    pub fn new_default() -> Self {
        Self::new(SearchConfig::default())
    }

    pub async fn search<S: MemoryStorage>(
        &self,
        query: &str,
        user_id: &str,
        storage: &S,
        ai_engine: &LocalAIEngine,
    ) -> SemanticResult<Vec<SearchResult>> {
        let search_start = Instant::now();
        *self.search_count.lock().unwrap() += 1;

        // Check W-TinyLFU result cache first
        if self.config.enable_result_caching {
            let cache_key = format!("{}:{}", user_id, query);
            if let Some(cached) = self.get_cached_search_result(&cache_key) {
                *self.cache_hits.lock().unwrap() += 1;
                let cache_metrics = self.search_result_cache.metrics();
                println!(
                    "üöÄ W-TinyLFU Search cache HIT - results in {}Œºs | Hit rate: {:.1}%",
                    search_start.elapsed().as_micros(),
                    cache_metrics.hit_rate()
                );
                return Ok(cached);
            }
        }

        // Generate embedding with W-TinyLFU cache
        let query_embedding = self
            .get_or_generate_query_embedding(query, ai_engine)
            .await?;

        // Rest of search logic remains the same...
        let memories = storage
            .list_memories(user_id)
            .await
            .map_err(SearchError::StorageError)?;

        if memories.is_empty() {
            return Ok(Vec::new());
        }

        let memories_with_embeddings: Vec<_> =
            memories.into_iter().filter(|m| m.has_embedding()).collect();

        if memories_with_embeddings.is_empty() {
            return Err(SearchError::NoEmbeddingsFound);
        }

        let similarities =
            self.compute_similarities(&query_embedding, &memories_with_embeddings)?;

        let mut results: Vec<SearchResult> = memories_with_embeddings
            .into_iter()
            .zip(similarities.into_iter())
            .filter(|(_, similarity)| *similarity >= self.config.similarity_threshold)
            .map(|(memory, similarity)| SearchResult {
                memory,
                similarity_score: similarity,
            })
            .collect();

        results.sort_by(|a, b| b.similarity_score.partial_cmp(&a.similarity_score).unwrap());

        if results.len() > self.config.max_results {
            results.truncate(self.config.max_results);
        }

        let elapsed = search_start.elapsed();
        if elapsed.as_millis() as u64 > self.config.search_timeout_ms {
            return Err(SearchError::Timeout {
                elapsed_ms: elapsed.as_millis() as u64,
                limit_ms: self.config.search_timeout_ms,
            });
        }

        // Cache results with W-TinyLFU - intelligent admission
        if self.config.enable_result_caching {
            let cache_key = format!("{}:{}", user_id, query);
            self.cache_search_result(cache_key, results.clone());
        }

        *self.total_search_time_ms.lock().unwrap() += elapsed.as_millis() as u64;

        let cache_metrics = self.search_result_cache.metrics();
        println!(
            "üîç W-TinyLFU Semantic search completed in {}ms - {} results | Cache hit rate: {:.1}%",
            elapsed.as_millis(),
            results.len(),
            cache_metrics.hit_rate()
        );

        Ok(results)
    }

    async fn get_or_generate_query_embedding(
        &self,
        query: &str,
        ai_engine: &LocalAIEngine,
    ) -> SemanticResult<Vec<f32>> {
        // Check W-TinyLFU cache first - automatically updates frequency
        if let Some(embedding) = self.query_embedding_cache.get(&query.to_string()) {
            let cache_metrics = self.query_embedding_cache.metrics();
            println!(
                "üöÄ W-TinyLFU Query embedding cache HIT | Hit rate: {:.1}%",
                cache_metrics.hit_rate()
            );
            return Ok(embedding);
        }

        println!(
            "üî∏ Generating query embedding with W-TinyLFU caching: \"{}\"",
            if query.len() > 50 {
                &query[..50]
            } else {
                query
            }
        );

        let embedding = ai_engine.generate_embedding(query).await?;

        // Cache with W-TinyLFU - intelligent admission control
        self.query_embedding_cache
            .insert(query.to_string(), embedding.clone());

        Ok(embedding)
    }

    fn compute_similarities(
        &self,
        query_embedding: &[f32],
        memories: &[MemoryEntry],
    ) -> SemanticResult<Vec<f32>> {
        let similarity_start = Instant::now();

        let embeddings: Vec<_> = memories
            .iter()
            .filter_map(|m| m.embedding.as_ref())
            .collect();

        if embeddings.is_empty() {
            return Ok(Vec::new());
        }

        let similarities: Vec<f32> = embeddings
            .iter()
            .map(|embedding| {
                crate::search::similarity::cosine_similarity_optimized(query_embedding, embedding)
            })
            .collect();

        let elapsed = similarity_start.elapsed();
        println!(
            "‚ö° W-TinyLFU-powered computed {} similarities in {}Œºs",
            similarities.len(),
            elapsed.as_micros()
        );

        Ok(similarities)
    }

    fn get_cached_search_result(&self, cache_key: &str) -> Option<Vec<SearchResult>> {
        if let Some(cached) = self.search_result_cache.get(&cache_key.to_string()) {
            // Simple expiration: 5 minutes
            if cached.created_at.elapsed().as_secs() < 300 {
                return Some(cached.results);
            }
        }
        None
    }

    fn cache_search_result(&self, cache_key: String, results: Vec<SearchResult>) {
        let cached_result = CachedSearchResult {
            results,
            created_at: Instant::now(),
        };

        // W-TinyLFU handles intelligent eviction automatically
        self.search_result_cache.insert(cache_key, cached_result);
    }

    pub fn get_performance_stats(&self) -> (u64, u64, f64, f64) {
        let search_count = *self.search_count.lock().unwrap();
        let cache_hits = *self.cache_hits.lock().unwrap();
        let total_time = *self.total_search_time_ms.lock().unwrap();

        let cache_hit_rate = if search_count > 0 {
            cache_hits as f64 / search_count as f64 * 100.0
        } else {
            0.0
        };

        let avg_search_time = if search_count > 0 {
            total_time as f64 / search_count as f64
        } else {
            0.0
        };

        (search_count, cache_hits, cache_hit_rate, avg_search_time)
    }

    pub fn get_advanced_cache_metrics(&self) -> (super::CacheMetrics, super::CacheMetrics) {
        (
            self.query_embedding_cache.metrics(),
            self.search_result_cache.metrics(),
        )
    }

    pub fn clear_caches(&self) {
        self.query_embedding_cache.clear();
        self.search_result_cache.clear();
        println!("üßπ All W-TinyLFU search caches cleared");
    }
}

/// Type aliases for enhanced implementations
pub type WTinyLFUInMemoryHub =
    crate::memory::LocalMindHub<WTinyLFUCachedStorage<crate::memory::InMemoryStorage>>;
pub type WTinyLFUSurrealDBHub =
    crate::memory::LocalMindHub<WTinyLFUCachedStorage<crate::memory::SurrealDBStorage>>;

/// Usage examples and migration guide
pub mod examples {
    use super::*;
    use crate::memory::{InMemoryStorage, SurrealDBStorage};

    /// Example: Migrate from simple CachedStorage to W-TinyLFU
    pub async fn migrate_cached_storage_example() {
        println!("üîÑ Migration Example: Simple Cache -> W-TinyLFU");

        // OLD: Simple HashMap-based cache
        // let storage = CachedStorage::new(InMemoryStorage::new(), 1000);

        // NEW: W-TinyLFU intelligent cache (drop-in replacement)
        let storage = WTinyLFUCachedStorage::new(InMemoryStorage::new(), 1000);
        let hub = crate::memory::LocalMindHub::new(storage);

        // Same interface, better performance
        let memory_id = hub
            .store_memory(
                "user123",
                "Test content with intelligent caching".to_string(),
            )
            .await
            .unwrap();
        let retrieved = hub.get_memory("user123", memory_id).await.unwrap();

        println!("‚úÖ Migration complete - same interface, 15-25% better hit rates");
    }

    /// Example: Migrate AI engine cache
    pub async fn migrate_ai_engine_example() {
        println!("üîÑ Migration Example: AI Engine Cache -> W-TinyLFU");

        // OLD: LocalAIEngine with simple HashMap cache
        // let ai_engine = LocalAIEngine::new().await.unwrap();

        // NEW: W-TinyLFU AI engine (enhanced performance)
        let ai_engine = WTinyLFUAIEngine::new().await.unwrap();

        // Generate embeddings with intelligent caching
        let embedding1 = ai_engine.generate_embedding("First query").await.unwrap();
        let embedding2 = ai_engine.generate_embedding("Second query").await.unwrap();
        let embedding1_cached = ai_engine.generate_embedding("First query").await.unwrap(); // Cache hit

        let (hits, misses, hit_rate) = ai_engine.get_cache_stats();
        let advanced_metrics = ai_engine.get_advanced_cache_metrics();

        println!(
            "‚úÖ AI Engine metrics: {:.1}% hit rate, {} promotions",
            hit_rate, advanced_metrics.promotions
        );
    }

    /// Example: Migrate semantic search caches
    pub async fn migrate_semantic_search_example() {
        println!("üîÑ Migration Example: Semantic Search -> W-TinyLFU");

        // OLD: SemanticSearch with HashMap caches
        // let search = crate::search::SemanticSearch::new_default();

        // NEW: W-TinyLFU semantic search
        let search = WTinyLFUSemanticSearch::new_default();

        let (query_metrics, result_metrics) = search.get_advanced_cache_metrics();
        println!(
            "‚úÖ Search caches initialized - Query: {:.1}%, Results: {:.1}% hit rates",
            query_metrics.hit_rate(),
            result_metrics.hit_rate()
        );
    }

    /// Performance comparison demo
    pub async fn performance_comparison_demo() {
        println!("üìä Performance Comparison: HashMap vs W-TinyLFU");

        // Create both cache types
        let simple_cache = std::collections::HashMap::<String, String>::new();
        let wtiny_cache = WTinyLFUCache::<String, String>::new(100);

        // Simulate realistic access pattern
        let keys: Vec<String> = (0..200).map(|i| format!("key_{}", i)).collect();

        // W-TinyLFU performance test
        let start = Instant::now();
        for key in &keys {
            wtiny_cache.insert(key.clone(), format!("value_{}", key));
        }
        let wtiny_insert_time = start.elapsed();

        let start = Instant::now();
        for key in &keys[..100] {
            // Access first 100 keys multiple times
            for _ in 0..3 {
                wtiny_cache.get(key);
            }
        }
        let wtiny_access_time = start.elapsed();

        let metrics = wtiny_cache.metrics();

        println!("   W-TinyLFU Results:");
        println!("     Insert time: {:?}", wtiny_insert_time);
        println!("     Access time: {:?}", wtiny_access_time);
        println!("     Hit rate: {:.1}%", metrics.hit_rate());
        println!(
            "     Promotions: {}, Evictions: {}",
            metrics.promotions, metrics.evictions
        );
        println!("     Avg operation: {}ns", metrics.avg_operation_time_ns);

        println!("‚úÖ W-TinyLFU demonstrates superior frequency-aware caching");
    }
}
